<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Favicon Icon -->
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.png">

    <title>Dog breed image classification with Keras</title>
    <meta name="description"
          content="What if you have a very small dataset of only a few thousand images and a hard classification problem at hand? Training a network from scratch might not work...">

    <link rel="canonical" href="http://machinememos.com/python/keras/artificial%20intelligence/machine%20learning/transfer%20learning/dog%20breed/neural%20networks/convolutional%20neural%20network/tensorflow/image%20classification/imagenet/2017/07/11/dog-breed-image-classification.html">
    <link rel="alternate" type="application/rss+xml" title="Blog" href="http://machinememos.com/feed.xml">

    <script type="text/javascript" src="/bower_components/jquery/dist/jquery.min.js"></script>

    <!-- Third-Party CSS -->
    <link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="/bower_components/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/bower_components/hover/css/hover-min.css">
    <link rel="stylesheet" href="/bower_components/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- My CSS -->
    <link rel="stylesheet" href="/assets/css/common.css">

    <!-- CSS set in page -->
    

    <!-- CSS set in layout -->
    
    <link rel="stylesheet" href="/assets/css/sidebar-post-nav.css">
    

    <script type="text/javascript" src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>

</head>


    <body>

    <header class="site-header">
    <div class="container">
        <a id="site-header-brand" href="/" title="MACHINE MEMOS">
			<div class="media">
    			<span class="media-left">
        			<img src="/assets/images/machine_memos_logo.svg" onerror="this.onerror=null; this.src='/assets/images/machine_memos_logo.png'" class="img-responsive" style="max-width: 1em;">
    			</span>
    			<div class="media-body" style="max-width:9em; vertical-align: middle;">
        			MACHINE MEMOS
				</div>
			</div>
			</a>
        <nav class="site-header-nav" role="navigation">
            
            <a href="/"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="BLOG">
                BLOG
            </a>
            
            <a href="/code"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="CODE">
                CODE
            </a>
            
            <a href="/timeline"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="TIMELINE">
                TIMELINE
            </a>
            
            <a href="/about"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="ABOUT">
                ABOUT
            </a>
            
        </nav>
    </div>
</header>


        <div class="content">
            <section class="jumbotron geopattern" data-pattern-id="Dog breed image classification with Keras">
    <div class="container">
        <div id="jumbotron-meta-info">
            <h1>Dog breed image classification with Keras</h1>
            <span class="meta-info">
                
                 
                <span class="octicon octicon-calendar"></span> 2017/07/11
                
				
					by Nikolay Kostadinov
				
            </span>
        </div>
    </div>
</section>
<script>
    $(document).ready(function(){

        $('.geopattern').each(function(){
            $(this).geopattern($(this).data('pattern-id'), {color:"#337ab7"});
        });

    });
</script>
<article class="post container" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="row">

        
        <div class="col-md-8 markdown-body">

            <p>What if you have a very small dataset of only a few thousand images and a hard classification problem at hand? Training a network from scratch might not work that well, but how about <a href="http://cs231n.github.io/transfer-learning/">transfer learning</a>.</p>

<h1 id="dog-breed-classification-with-keras">Dog Breed Classification with Keras</h1>

<p>Recently, I got my hands on a very interesting dataset that is part of the Udacity AI Nanodegree. In several of my previous posts I discussed the enormous potential of <a href="http://cs231n.github.io/transfer-learning/">transfer learning</a>. As a matter of fact, very few people train an entire convolutional network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pre-train a convolutional network on a very large dataset (e.g. <a href="http://www.image-net.org/">ImageNet</a>, which contains 1.2 million images with 1000 categories), and then use the convolutional network either as an initialization or a fixed feature extractor for the task of interest.</p>

<p>In this post, I aim to compare two approaches to image classification. First, I will train a convolutional neural network from scratch and measure its performance. Then, I will apply transfer learning and will create a stack of models and compare their performance to the first approach. For that purpose, I will use <a href="https://keras.io/">Keras</a>. While I got really comfortable at using Tensorflow, I must admit, using the high-level wrapper API that is Keras gets you much faster to the desired network architecture. Nevertheless, I still would recommend to every beginner to start with Tensorflow, as its low-level API really helps you understand how different types of neural networks work.</p>

<h2 id="dog-breed-dataset">Dog Breed Dataset</h2>
<p>The data consists of 8351 dog images. The images are sorted into 133 directories, each directory contains only images of a single dog breed. Hopefully, the dataset will stay <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">here</a>. If the url is not available, feel free to contact me. Ok, let’s load the dataset.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_files</span>       
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">np_utils</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>

<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">dog_files</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'filenames'</span><span class="p">])</span>
    <span class="n">dog_targets</span> <span class="o">=</span> <span class="n">np_utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]),</span> <span class="mi">133</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dog_files</span><span class="p">,</span> <span class="n">dog_targets</span>

<span class="n">train_files</span><span class="p">,</span> <span class="n">train_targets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">'dog/assets/images/train'</span><span class="p">)</span>
<span class="n">valid_files</span><span class="p">,</span> <span class="n">valid_targets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">'dog/assets/images/valid'</span><span class="p">)</span>
<span class="n">test_files</span><span class="p">,</span> <span class="n">test_targets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">'dog/assets/images/test'</span><span class="p">)</span>

<span class="n">dog_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="p">(</span><span class="s">"dog/assets/images/train/*/"</span><span class="p">))]</span>

<span class="c"># Let's check the dataset</span>
<span class="k">print</span><span class="p">(</span><span class="s">'There are </span><span class="si">%</span><span class="s">d total dog categories.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">dog_names</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'There are </span><span class="si">%</span><span class="s">s total dog images.</span><span class="se">\n</span><span class="s">'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">train_files</span><span class="p">,</span> <span class="n">valid_files</span><span class="p">,</span> <span class="n">test_files</span><span class="p">])))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'There are </span><span class="si">%</span><span class="s">d training dog images.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_files</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'There are </span><span class="si">%</span><span class="s">d validation dog images.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_files</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'There are </span><span class="si">%</span><span class="s">d test dog images.'</span><span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_files</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Using TensorFlow backend.

There are 133 total dog categories.
There are 8351 total dog images.

There are 6680 training dog images.
There are 835 validation dog images.
There are 836 test dog images.
</code></pre>
</div>

<p>The dataset is already split into train, validation and test parts. As the training set consists of 6680 images, there are only 50 dogs per breed on average. That is really a rather small dataset and an ambitious task to do. The <a href="https://www.cs.toronto.edu/~kriz/cifar.html">cifar 10 dataset</a> for example contains 60000 images and only 10 categories. The categories are airplane, automobile, bird, cat, etc. Thus, objects to be classified are very different and therefore easier to classify. In my post <a href="http://machinememos.com/python/artificial%20intelligence/machine%20learning/cifar10/neural%20networks/convolutional%20neural%20network/googlelenet/inception/tensorflow/dropout/image%20classification/2017/05/04/cnn-image-classification-cifar-10-inceptionV3.html">Image classification with pre-trained CNN InceptionV3</a> I managed to achieve an accuracy of around 80%. Hence, it is now my goal to achieve similar accuracy with the dog breed dataset, that has much more categories, while it is much much smaller.</p>

<h2 id="why-is-the-dataset-interesting">Why is the dataset interesting?</h2>

<p>The task of assigning breed to dogs from images is considered exceptionally challenging. To see why, consider that <em>even a human</em> would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.</p>

<table>
  <thead>
    <tr>
      <th>Brittany</th>
      <th>Welsh Springer Spaniel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/images/Brittany_02625.jpg" width="100" /></td>
      <td><img src="/assets/images/Welsh_springer_spaniel_08203.jpg" width="200" /></td>
    </tr>
  </tbody>
</table>

<p>It was not difficult to find other dog breed pairs with only few inter-class variations (for instance, Curly-Coated Retrievers and American Water Spaniels).</p>

<table>
  <thead>
    <tr>
      <th>Curly-Coated Retriever</th>
      <th>American Water Spaniel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/images/Curly-coated_retriever_03896.jpg" width="200" /></td>
      <td><img src="/assets/images/American_water_spaniel_00648.jpg" width="200" /></td>
    </tr>
  </tbody>
</table>

<p>Likewise, labradors come in yellow, chocolate, and black. A vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.</p>

<table>
  <thead>
    <tr>
      <th>Yellow Labrador</th>
      <th>Chocolate Labrador</th>
      <th>Black Labrador</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/images/Labrador_retriever_06457.jpg" width="150" /></td>
      <td><img src="/assets/images/Labrador_retriever_06455.jpg" width="240" /></td>
      <td><img src="/assets/images/Labrador_retriever_06449.jpg" width="220" /></td>
    </tr>
  </tbody>
</table>

<p>When predicting between 133 breeds, a random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imbalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%. Hence, even an accuracy of 2-3% would be considered reasonable.</p>

<h2 id="pre-process-the-data">Pre-process the Data</h2>

<p>When using TensorFlow as backend, Keras CNNs require a 4D array (which we’ll also refer to as a 4D tensor) as input, with shape</p>

<script type="math/tex; mode=display">(\text{nb_samples}, \text{rows}, \text{columns}, \text{channels}),</script>

<p>where <code class="highlighter-rouge">nb_samples</code> corresponds to the total number of images (or samples), and <code class="highlighter-rouge">rows</code>, <code class="highlighter-rouge">columns</code>, and <code class="highlighter-rouge">channels</code> correspond to the number of rows, columns, and channels for each image, respectively.</p>

<p>The <code class="highlighter-rouge">path_to_tensor</code> function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape</p>

<script type="math/tex; mode=display">(1, 224, 224, 3).</script>

<p>The <code class="highlighter-rouge">paths_to_tensor</code> function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape</p>

<script type="math/tex; mode=display">(\text{nb_samples}, 224, 224, 3).</script>

<p>Here, <code class="highlighter-rouge">nb_samples</code> is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of <code class="highlighter-rouge">nb_samples</code> as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span>                  
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">def</span> <span class="nf">path_to_tensor</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c"># loads RGB image as PIL.Image.Image type</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="n">img_path</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
    <span class="c"># convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="c"># convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">paths_to_tensor</span><span class="p">(</span><span class="n">img_paths</span><span class="p">):</span>
    <span class="n">list_of_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">path_to_tensor</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span> <span class="k">for</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">img_paths</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">list_of_tensors</span><span class="p">)</span>
</code></pre>
</div>

<p>We rescale the images by dividing every pixel in every image by 255.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">ImageFile</span>                            
<span class="n">ImageFile</span><span class="o">.</span><span class="n">LOAD_TRUNCATED_IMAGES</span> <span class="o">=</span> <span class="bp">True</span>                 

<span class="c"># pre-process the data for Keras</span>
<span class="n">train_tensors</span> <span class="o">=</span> <span class="n">paths_to_tensor</span><span class="p">(</span><span class="n">train_files</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
<span class="n">valid_tensors</span> <span class="o">=</span> <span class="n">paths_to_tensor</span><span class="p">(</span><span class="n">valid_files</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
<span class="n">test_tensors</span> <span class="o">=</span> <span class="n">paths_to_tensor</span><span class="p">(</span><span class="n">test_files</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span><span class="o">/</span><span class="mi">255</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>100%|██████████| 6680/6680 [01:00&lt;00:00, 110.84it/s]
100%|██████████| 835/835 [00:06&lt;00:00, 127.85it/s]
100%|██████████| 836/836 [00:06&lt;00:00, 136.50it/s]
</code></pre>
</div>

<h2 id="create-a-cnn-to-classify-dog-breeds-from-scratch">Create a CNN to Classify Dog Breeds (from Scratch)</h2>
<p>After few hours of trial and error, I came up with the following CNN architecture:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">GlobalAveragePooling2D</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.normalization</span> <span class="kn">import</span> <span class="n">BatchNormalization</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">133</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_46 (Conv2D)           (None, 224, 224, 16)      432       
_________________________________________________________________
batch_normalization_105 (Bat (None, 224, 224, 16)      48        
_________________________________________________________________
activation_1479 (Activation) (None, 224, 224, 16)      0         
_________________________________________________________________
max_pooling2d_66 (MaxPooling (None, 56, 56, 16)        0         
_________________________________________________________________
dropout_84 (Dropout)         (None, 56, 56, 16)        0         
_________________________________________________________________
conv2d_47 (Conv2D)           (None, 56, 56, 32)        4608      
_________________________________________________________________
batch_normalization_106 (Bat (None, 56, 56, 32)        96        
_________________________________________________________________
activation_1480 (Activation) (None, 56, 56, 32)        0         
_________________________________________________________________
max_pooling2d_67 (MaxPooling (None, 14, 14, 32)        0         
_________________________________________________________________
dropout_85 (Dropout)         (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_48 (Conv2D)           (None, 14, 14, 64)        18432     
_________________________________________________________________
batch_normalization_107 (Bat (None, 14, 14, 64)        192       
_________________________________________________________________
activation_1481 (Activation) (None, 14, 14, 64)        0         
_________________________________________________________________
max_pooling2d_68 (MaxPooling (None, 4, 4, 64)          0         
_________________________________________________________________
dropout_86 (Dropout)         (None, 4, 4, 64)          0         
_________________________________________________________________
conv2d_49 (Conv2D)           (None, 4, 4, 128)         73728     
_________________________________________________________________
batch_normalization_108 (Bat (None, 4, 4, 128)         384       
_________________________________________________________________
activation_1482 (Activation) (None, 4, 4, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 2048)              0         
_________________________________________________________________
dropout_87 (Dropout)         (None, 2048)              0         
_________________________________________________________________
dense_100 (Dense)            (None, 512)               1049088   
_________________________________________________________________
dense_101 (Dense)            (None, 133)               68229     
=================================================================
Total params: 1,215,237
Trainable params: 1,214,757
Non-trainable params: 480
_________________________________________________________________
</code></pre>
</div>

<p>As already elaborated, designing a CNN architecture that achieves even 2% accuracy is not an easy task. The first thing you notice is that increasing the filters depth leads to better results, yet slower training.<a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a> seems not only to lead to faster training, but also to better results. I used the <a href="https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py">source code of InceptionV3</a> as an example when configuring the batch normalization layers. As batch normalization allowed for the model to learn much faster and I added a fourth convolutional layer and further increased the filter depth. Then, I altered the max pooling layer to shrink the layers by a factor of x4 instead of x2. This drastically decreased the number of trainable params and increased the speed by which the model is learning. At the end I added Dropout, to decrease overfitting, as the network started to overfit after the 4th epoch.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>  

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s">'saved_models/weights.best.from_scratch.hdf5'</span><span class="p">,</span> 
                               <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_tensors</span><span class="p">,</span> <span class="n">train_targets</span><span class="p">,</span> 
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_tensors</span><span class="p">,</span> <span class="n">valid_targets</span><span class="p">),</span>
          <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Train on 6680 samples, validate on 835 samples
Epoch 1/10
6656/6680 [============================&gt;.] - ETA: 1s - loss: 4.8959 - acc: 0.0207Epoch 00000: val_loss improved from inf to 5.09728, saving model to saved_models/weights.best.from_scratch.hdf5
6680/6680 [==============================] - 434s - loss: 4.8946 - acc: 0.0207 - val_loss: 5.0973 - val_acc: 0.0156
Epoch 2/10
6656/6680 [============================&gt;.] - ETA: 0s - loss: 4.4014 - acc: 0.0524Epoch 00001: val_loss improved from 5.09728 to 4.45084, saving model to saved_models/weights.best.from_scratch.hdf5
6680/6680 [==============================] - 292s - loss: 4.4012 - acc: 0.0524 - val_loss: 4.4508 - val_acc: 0.0479
Epoch 3/10
6656/6680 [============================&gt;.] - ETA: 0s - loss: 4.1037 - acc: 0.0726Epoch 00002: val_loss did not improve
6680/6680 [==============================] - 274s - loss: 4.1032 - acc: 0.0731 - val_loss: 4.4804 - val_acc: 0.0443
Epoch 4/10
6656/6680 [============================&gt;.] - ETA: 0s - loss: 3.9247 - acc: 0.0959Epoch 00003: val_loss improved from 4.45084 to 4.43195, saving model to saved_models/weights.best.from_scratch.hdf5
6680/6680 [==============================] - 273s - loss: 3.9240 - acc: 0.0960 - val_loss: 4.4319 - val_acc: 0.0491
Epoch 5/10
6656/6680 [============================&gt;.] - ETA: 0s - loss: 3.7687 - acc: 0.1175Epoch 00004: val_loss did not improve
6680/6680 [==============================] - 276s - loss: 3.7678 - acc: 0.1175 - val_loss: 4.9665 - val_acc: 0.0347
Epoch 6/10
6656/6680 [============================&gt;.] - ETA: 0s - loss: 3.6533 - acc: 0.1315Epoch 00005: val_loss did not improve
6680/6680 [==============================] - 293s - loss: 3.6520 - acc: 0.1317 - val_loss: 4.6552 - val_acc: 0.0671
Epoch 7/10
6656/6680 [============================&gt;.] - ETA: 1s - loss: 3.5410 - acc: 0.1513Epoch 00006: val_loss improved from 4.43195 to 4.18182, saving model to saved_models/weights.best.from_scratch.hdf5
6680/6680 [==============================] - 313s - loss: 3.5407 - acc: 0.1510 - val_loss: 4.1818 - val_acc: 0.0743
Epoch 8/10
6656/6680 [============================&gt;.] - ETA: 1s - loss: 3.4297 - acc: 0.1773Epoch 00007: val_loss improved from 4.18182 to 4.05759, saving model to saved_models/weights.best.from_scratch.hdf5
6680/6680 [==============================] - 310s - loss: 3.4286 - acc: 0.1772 - val_loss: 4.0576 - val_acc: 0.1066
Epoch 9/10
6656/6680 [============================&gt;.] - ETA: 1s - loss: 3.3242 - acc: 0.1881Epoch 00008: val_loss did not improve
6680/6680 [==============================] - 297s - loss: 3.3236 - acc: 0.1883 - val_loss: 4.4697 - val_acc: 0.0683
Epoch 10/10
6656/6680 [============================&gt;.] - ETA: 1s - loss: 3.1783 - acc: 0.2160Epoch 00009: val_loss did not improve
6680/6680 [==============================] - 300s - loss: 3.1793 - acc: 0.2156 - val_loss: 4.2501 - val_acc: 0.1006
</code></pre>
</div>

<p>Running the model for 10 epochs took less than an hour, when running on 8-Core CPU. Meanwhile, I am using <a href="https://www.floydhub.com/">Floyd Hub</a> to rent a GPU when considerably more power is required. In mostly works fine, once you manage to upload your dataset (their upload pipeline is currently buggy). Let’s load the weights of the model that had the best validation loss and measure the accuracy.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'saved_models/weights.best.from_scratch.hdf5'</span><span class="p">)</span>
<span class="c"># get index of predicted dog breed for each image in test set</span>
<span class="n">dog_breed_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">test_tensors</span><span class="p">]</span>
<span class="c"># report test accuracy</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dog_breed_predictions</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_targets</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">dog_breed_predictions</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy: </span><span class="si">%.4</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span> <span class="o">%</span> <span class="n">test_accuracy</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test accuracy: 11.1244%
</code></pre>
</div>

<p>That is not a bad performance. Probably as good as someone that is not an expert, but really likes dogs would manage to achieve.</p>

<h2 id="using-pre-trained-vgg-19-and-resnet-50">Using pre-trained VGG-19 and Resnet-50</h2>

<p>Next, we will use transfer learning to create a CNN that can identify dog breed from images. The model uses the pre-trained <a href="https://github.com/fchollet/keras/blob/master/keras/applications/vgg19.py">VGG-19</a> and <a href="https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py">Resnet-50</a> models as a fixed feature extractor, where the last convolutional output of both networks is fed as input to another, second level model. As a matter of fact, one can choose between several pre-trained models that are shipped with Keras. I have already tested VGG-16, VGG-19, InceptionV3, Resnet-50 and Xception on this dataset and found VGG-19 and Resnet-50 to have the best performance considering the limited memory resources and training time that I had at my disposal. At the end, I combined both models to achieve a small boost relative to what I achieved by using them separately. Here are few lines, that extract the features from the images:</p>

<p>We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax. Let’s extract the last convolutional output for both networks.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.applications.vgg19</span> <span class="kn">import</span> <span class="n">VGG19</span>
<span class="kn">from</span> <span class="nn">keras.applications.vgg19</span> <span class="kn">import</span> <span class="n">preprocess_input</span> <span class="k">as</span> <span class="n">preprocess_input_vgg19</span>
<span class="kn">from</span> <span class="nn">keras.applications.resnet50</span> <span class="kn">import</span> <span class="n">ResNet50</span>
<span class="kn">from</span> <span class="nn">keras.applications.resnet50</span> <span class="kn">import</span> <span class="n">preprocess_input</span> <span class="k">as</span> <span class="n">preprocess_input_resnet50</span>

<span class="k">def</span> <span class="nf">extract_VGG19</span><span class="p">(</span><span class="n">file_paths</span><span class="p">):</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="n">paths_to_tensor</span><span class="p">(</span><span class="n">file_paths</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
    <span class="n">preprocessed_input</span> <span class="o">=</span> <span class="n">preprocess_input_vgg19</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">VGG19</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">preprocessed_input</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">extract_Resnet50</span><span class="p">(</span><span class="n">file_paths</span><span class="p">):</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="n">paths_to_tensor</span><span class="p">(</span><span class="n">file_paths</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
    <span class="n">preprocessed_input</span> <span class="o">=</span> <span class="n">preprocess_input_resnet50</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResNet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">preprocessed_input</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</code></pre>
</div>

<p>Extracting the features may take a few minutes…</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">train_vgg19</span> <span class="o">=</span> <span class="n">extract_VGG19</span><span class="p">(</span><span class="n">train_files</span><span class="p">)</span>
<span class="n">valid_vgg19</span> <span class="o">=</span> <span class="n">extract_VGG19</span><span class="p">(</span><span class="n">valid_files</span><span class="p">)</span>
<span class="n">test_vgg19</span> <span class="o">=</span> <span class="n">extract_VGG19</span><span class="p">(</span><span class="n">test_files</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"VGG19 shape"</span><span class="p">,</span> <span class="n">train_vgg19</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="n">train_resnet50</span> <span class="o">=</span> <span class="n">extract_Resnet50</span><span class="p">(</span><span class="n">train_files</span><span class="p">)</span>
<span class="n">valid_resnet50</span> <span class="o">=</span> <span class="n">extract_Resnet50</span><span class="p">(</span><span class="n">valid_files</span><span class="p">)</span>
<span class="n">test_resnet50</span> <span class="o">=</span> <span class="n">extract_Resnet50</span><span class="p">(</span><span class="n">test_files</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Resnet50 shape"</span><span class="p">,</span> <span class="n">train_resnet50</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>VGG19 shape (7, 7, 512)
Resnet50 shape (1, 1, 2048)
</code></pre>
</div>

<p>For the second level model, <a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a> yet again proved to be very important. Without batch normalization the model will not reach 80% accuracy for 10 epochs. Dropout is also important as it allows for the model to train more epochs before starting to overfit. However a Dropout of 50% leads to a model that trains all 20 epochs without overfitting, yet does not reach 82% accuracy. I’ve found Dropout of 30% to be just right for the model below. Another important hyper parameter was the batch size. A bigger batch size leads to a model that learns faster, the accuracy increases very rapidly, but the maximum accuracy is a bit lower. A smaller batch size leads to a model that learns slower between epochs but reaches higher accuracy.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers.pooling</span> <span class="kn">import</span> <span class="n">GlobalAveragePooling2D</span>
<span class="kn">from</span> <span class="nn">keras.layers.merge</span> <span class="kn">import</span> <span class="n">Concatenate</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">keras.layers.normalization</span> <span class="kn">import</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="k">def</span> <span class="nf">input_branch</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
    <span class="n">size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
    
    <span class="n">branch_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">branch</span> <span class="o">=</span> <span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">branch_input</span><span class="p">)</span>
    <span class="n">branch</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'uniform'</span><span class="p">)(</span><span class="n">branch</span><span class="p">)</span>
    <span class="n">branch</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">branch</span><span class="p">)</span>
    <span class="n">branch</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">branch</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">branch</span><span class="p">,</span> <span class="n">branch_input</span>

<span class="n">vgg19_branch</span><span class="p">,</span> <span class="n">vgg19_input</span> <span class="o">=</span> <span class="n">input_branch</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">resnet50_branch</span><span class="p">,</span> <span class="n">resnet50_input</span> <span class="o">=</span> <span class="n">input_branch</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">))</span>
<span class="n">concatenate_branches</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()([</span><span class="n">vgg19_branch</span><span class="p">,</span> <span class="n">resnet50_branch</span><span class="p">])</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">concatenate_branches</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'uniform'</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">net</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">133</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'uniform'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">vgg19_input</span><span class="p">,</span> <span class="n">resnet50_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">net</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 7, 7, 512)     0                                            
____________________________________________________________________________________________________
input_4 (InputLayer)             (None, 1, 1, 2048)    0                                            
____________________________________________________________________________________________________
global_average_pooling2d_3 (Glob (None, 512)           0           input_3[0][0]                    
____________________________________________________________________________________________________
global_average_pooling2d_4 (Glob (None, 2048)          0           input_4[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 128)           65536       global_average_pooling2d_3[0][0] 
____________________________________________________________________________________________________
dense_6 (Dense)                  (None, 512)           1048576     global_average_pooling2d_4[0][0] 
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 128)           512         dense_5[0][0]                    
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 512)           2048        dense_6[0][0]                    
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 128)           0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 512)           0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
concatenate_2 (Concatenate)      (None, 640)           0           activation_4[0][0]               
                                                                   activation_5[0][0]               
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 640)           0           concatenate_2[0][0]              
____________________________________________________________________________________________________
dense_7 (Dense)                  (None, 640)           409600      dropout_3[0][0]                  
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 640)           2560        dense_7[0][0]                    
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 640)           0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 640)           0           activation_6[0][0]               
____________________________________________________________________________________________________
dense_8 (Dense)                  (None, 133)           85253       dropout_4[0][0]                  
====================================================================================================
Total params: 1,614,085
Trainable params: 1,611,525
Non-trainable params: 2,560
____________________________________________________________________________________________________
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"rmsprop"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">checkpointer</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s">'saved_models/bestmodel.hdf5'</span><span class="p">,</span> 
                               <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">train_vgg19</span><span class="p">,</span> <span class="n">train_resnet50</span><span class="p">],</span> <span class="n">train_targets</span><span class="p">,</span> 
          <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">valid_vgg19</span><span class="p">,</span> <span class="n">valid_resnet50</span><span class="p">],</span> <span class="n">valid_targets</span><span class="p">),</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpointer</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Train on 6680 samples, validate on 835 samples
Epoch 1/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 2.5900 - acc: 0.3751Epoch 00000: val_loss improved from inf to 1.06250, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 69s - loss: 2.5887 - acc: 0.3754 - val_loss: 1.0625 - val_acc: 0.6838
Epoch 2/10
6672/6680 [============================&gt;.] - ETA: 0s - loss: 1.5383 - acc: 0.5679Epoch 00001: val_loss improved from 1.06250 to 0.87527, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 50s - loss: 1.5376 - acc: 0.5680 - val_loss: 0.8753 - val_acc: 0.7401
Epoch 3/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 1.3559 - acc: 0.6257Epoch 00002: val_loss improved from 0.87527 to 0.79809, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 50s - loss: 1.3568 - acc: 0.6256 - val_loss: 0.7981 - val_acc: 0.7784
Epoch 4/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 1.2502 - acc: 0.6552Epoch 00003: val_loss improved from 0.79809 to 0.74536, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 49s - loss: 1.2503 - acc: 0.6551 - val_loss: 0.7454 - val_acc: 0.8012
Epoch 5/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 1.1436 - acc: 0.6824Epoch 00004: val_loss did not improve
6680/6680 [==============================] - 49s - loss: 1.1438 - acc: 0.6823 - val_loss: 0.7806 - val_acc: 0.8084
Epoch 6/10
6672/6680 [============================&gt;.] - ETA: 0s - loss: 1.0829 - acc: 0.7052Epoch 00005: val_loss improved from 0.74536 to 0.72584, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 49s - loss: 1.0820 - acc: 0.7054 - val_loss: 0.7258 - val_acc: 0.8024
Epoch 7/10
6672/6680 [============================&gt;.] - ETA: 0s - loss: 1.0586 - acc: 0.7136Epoch 00006: val_loss did not improve
6680/6680 [==============================] - 48s - loss: 1.0578 - acc: 0.7136 - val_loss: 0.7493 - val_acc: 0.8072
Epoch 8/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 1.0034 - acc: 0.7218Epoch 00007: val_loss did not improve
6680/6680 [==============================] - 52s - loss: 1.0041 - acc: 0.7217 - val_loss: 0.7958 - val_acc: 0.8120
Epoch 9/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 0.9489 - acc: 0.7326Epoch 00008: val_loss improved from 0.72584 to 0.72160, saving model to saved_models/bestmodel.hdf5
6680/6680 [==============================] - 47s - loss: 0.9484 - acc: 0.7328 - val_loss: 0.7216 - val_acc: 0.8228
Epoch 10/10
6676/6680 [============================&gt;.] - ETA: 0s - loss: 0.9080 - acc: 0.7431Epoch 00009: val_loss did not improve
6680/6680 [==============================] - 47s - loss: 0.9076 - acc: 0.7433 - val_loss: 0.7365 - val_acc: 0.8228
</code></pre>
</div>

<p>Training the model takes only a few minutes… Let’s load the weights of the model that had the best validation loss and measure the accuracy.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'saved_models/bestmodel.hdf5'</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">test_vgg19</span><span class="p">,</span> <span class="n">test_resnet50</span><span class="p">])</span>
<span class="n">breed_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span> <span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">]</span>
<span class="n">breed_true_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">true_label</span><span class="p">)</span> <span class="k">for</span> <span class="n">true_label</span> <span class="ow">in</span> <span class="n">test_targets</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy: </span><span class="si">%.4</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">breed_true_labels</span><span class="p">,</span> <span class="n">breed_predictions</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Test accuracy: 82.2967%
</code></pre>
</div>

<p>The accuracy, when tested on the test set, is 82.3%. I find it really impressive compared to the 11% accuracy achieved by the model that was trained from scratch. The reason the accuracy is so much higher is that both VGG-19 and Resnet-50 were trained on <a href="http://www.image-net.org/">ImageNet</a>, which is not only huge (1.2 million images), but also contains a considerable amount of dog images. As a result, the accuracy achieved by using models pre-trained on ImageNet is much higher than the accuracy that could possibly be achieved by training a model from scratch. Well, <a href="http://www.andrewng.org/">Andrew Ng</a>, the founder of <a href="http://coursera.org/">Coursera</a> and one of the biggest names in the ML realm said during his widely popular NIPS 2016 tutorial that transfer learning will be the next driver of ML commercial success. I can imagine, that in the future, models pre-trained on massive datasets would be made available by Google, Apple, Amazon, and others in exchange for some kind of subscription fee or some other form of payment. As a result, data scientists would be able to achieve remarkable results even when only provided with a limited set of data to use for training.</p>

<p>As always feel free to contact me or check out and execute the whole jupyter notebook: <a href="https://github.com/n-kostadinov/keras-dogbreed">Dog Breed Github Repo</a></p>


            <!-- Comments -->
            
<div class="comments">
    <div id="disqus_thread"></div>
    <script>
        /**
         * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
         */
        /*
         var disqus_config = function () {
         this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
         this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
         };
         */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');

            s.src = '//machinememos-com/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


        </div>

        <div class="col-md-4">
            <h3>Post Directory</h3>
<div id="post-directory-module">
<section class="post-directory">
    <!-- Links that trigger the jumping -->
    <!-- Added by javascript below -->
    <dl></dl>
</section>
</div>

<script type="text/javascript">

    $(document).ready(function(){
        $( "article h2" ).each(function( index ) {
            $(".post-directory dl").append("<dt><a class=\"jumper\" hre=#" +
                    $(this).attr("id")
                    + ">"
                    + $(this).text()
                    + "</a></dt>");

            var children = $(this).nextUntil("h2", "h3")

            children.each(function( index ) {
                $(".post-directory dl").append("<dd><a class=\"jumper\" hre=#" +
                        $(this).attr("id")
                        + ">"
                        + "&nbsp;&nbsp;- " + $(this).text()
                        + "</a></dd>");
            });
        });

        var fixmeTop = $('#post-directory-module').offset().top - 100;       // get initial position of the element

        $(window).scroll(function() {                  // assign scroll event listener

            var currentScroll = $(window).scrollTop(); // get current position

            if (currentScroll >= fixmeTop) {           // apply position: fixed if you
                $('#post-directory-module').css({      // scroll to that element or below it
                    top: '100px',
                    position: 'fixed',
                    width: 'inherit'
                });
            } else {                                   // apply position: static
                $('#post-directory-module').css({      // if you scroll above it
                    position: 'inherit',
                    width: 'inherit'
                });
            }

        });

        $("a.jumper").on("click", function( e ) {

            e.preventDefault();

            $("body, html").animate({
                scrollTop: ($( $(this).attr('hre') ).offset().top - 100)
            }, 600);

        });
    });

</script>
        </div>
        

    </div>

</article>

        </div>

    <footer class="container">

    <div class="site-footer">

        <div class="copyright pull-left">
            <!-- 请不要更改这一行 方便其他人知道模板的来源 谢谢 -->
            <!-- Please keep this line to let others know where this theme comes from. Thank you :D -->
            Power by <a href="https://github.com/DONGChuan/Yummy-Jekyll">Yummy Jekyll</a>
        </div>

        <a href="https://github.com/DONGChuan" target="_blank" aria-label="view source code">
            <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
        </a>

        <div class="pull-right">
            <a href="javascript:window.scrollTo(0,0)" >TOP</a>
        </div>

    </div>

    <!-- Third-Party JS -->
    <script type="text/javascript" src="/bower_components/geopattern/js/geopattern.min.js"></script>

    <!-- My JS -->
    <script type="text/javascript" src="/assets/js/script.js"></script>

    

    
    <!-- Google Analytics -->
    <div style="display:none">
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-26182347-1', 'auto');
            ga('send', 'pageview');

        </script>
    </div>
    

</footer>


    </body>

</html>
